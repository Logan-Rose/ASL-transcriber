{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "import skimage\n",
    "import cv2 as cv\n",
    "from cv2 import THRESH_OTSU\n",
    "import mediapipe as mp\n",
    "from datetime import datetime\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
    "import skimage\n",
    "from skimage import color\n",
    "from skimage.transform import rescale, resize\n",
    "\n",
    "# for reproducibility purposes\n",
    "SEED = 123\n",
    "tf.random.set_seed(SEED)\n",
    "classes=['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "\n",
    "# load tensorboard extension\n",
    "%reload_ext tensorboard\n",
    "# specify the log directory where the tensorboard logs will be written\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes hand detection output to add bounding rectangles\n",
    "def find_hands(current, width, height):\n",
    "    rect_list = []\n",
    "    hand_classifier = []\n",
    "    center = (0,0)\n",
    "    # iterates through hand tracking data per hand\n",
    "    if current.multi_hand_landmarks:\n",
    "        for index, info in enumerate(current.multi_handedness):\n",
    "            which = info.classification[0].label\n",
    "            hand_classifier.append(which)\n",
    "        for index, hand in enumerate(current.multi_hand_landmarks):\n",
    "            # values from 0 to 1 converted into values corresponding to video size\n",
    "            hand_list = []\n",
    "            for individual in hand.landmark:\n",
    "                hand_list.append((int(individual.x * width), int(individual.y * height), int(individual.z * width)))\n",
    "            # developing bounding box coordinates\n",
    "            x_values = np.array(hand_list)[:, 0]\n",
    "            y_values = np.array(hand_list)[:, 1]\n",
    "            x_min = int(np.min(x_values) - 10)\n",
    "            y_min = int(np.min(y_values) - 10)\n",
    "            x_max = int(np.max(x_values) + 10)\n",
    "            y_max = int(np.max(y_values) + 10)\n",
    "            center = (x_min + (x_max-x_min)//2, y_min + (y_max-y_min)//2)\n",
    "            rect_list.append(((x_min, y_min), (x_max, y_max), (0, 255, 0), hand_classifier[index]))\n",
    "    return rect_list, center\n",
    "def capture():\n",
    "    video = cv.VideoCapture(0, cv.CAP_DSHOW) #captureDevice = camera\n",
    "    running, original = video.read()\n",
    "    h, w, _ = original.shape    \n",
    "    # machine learning algorithm (using mediapipe, via google)\n",
    "    # init_hands = mp.solutions.hands\n",
    "    # hands = init_hands.Hands()\n",
    "    hands = mp.solutions.hands.Hands()\n",
    "    savedFrames = []\n",
    "    # continuous looping\n",
    "    while True:\n",
    "        running, original = video.read()\n",
    "        if not running:\n",
    "            break\n",
    "        # hand processing\n",
    "        track_curr = hands.process(original)\n",
    "        # calculating bounding rectangles\n",
    "        current_rects, center = find_hands(track_curr, w, h)\n",
    "\n",
    "        if current_rects:\n",
    "          if len(current_rects) > 1:\n",
    "            print('Please use only one hand')\n",
    "          else:\n",
    "            current_rect = current_rects[0]\n",
    "            left = current_rect[0][0]\n",
    "            right = current_rect[1][0]\n",
    "            top = current_rect[0][1]\n",
    "            bottom = current_rect[1][1]\n",
    "            hand_width = right - left\n",
    "            hand_height = bottom-top\n",
    "            square_top_left = (center[0]-75,center[1]-75)\n",
    "            square_bottom_right = (center[0]+75,center[1]+75)\n",
    "\n",
    "            red = (0,0,255)\n",
    "            green = (0,255,0)\n",
    "            if hand_width <= 150 and hand_height <= 150:\n",
    "              if top > 0 and left > 0 and bottom > 0 and right > 0:\n",
    "                cv.rectangle(original, current_rect[0], current_rect[1], green, 1)\n",
    "                # cv.rectangle(original,square_top_left ,square_bottom_right , green,1)\n",
    "                crop = original[center[1]-75:center[1]+75,center[0]-75:center[0]+75]   \n",
    "                savedFrames.append(crop)\n",
    "              \n",
    "                # cv.circle(original, center, 10, 2, 2)\n",
    "            else:\n",
    "              cv.rectangle(original, current_rect[0], current_rect[1], red, 2)\n",
    "              cv.rectangle(original,square_top_left ,square_bottom_right , red,2)\n",
    "              print(\"You're too close! Back up!\")\n",
    "        cv.imshow(\"Video Feed\", original)\n",
    "        k = cv.waitKey(1) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "    cv.destroyAllWindows()\n",
    "    video.release()\n",
    "    return savedFrames\n",
    "frames = capture()\n",
    "def open_operation(ex_img):\n",
    "    ex_img = cv.erode(ex_img, (11, 11))\n",
    "    ex_img = cv.dilate(ex_img, (11, 11))\n",
    "    return ex_img\n",
    "def prepare_image(grayImg):\n",
    "    img1 = cv.medianBlur(grayImg, 13)\n",
    "    ex_img = open_operation(img1)\n",
    "    ex_img = open_operation(ex_img)\n",
    "    ex_img = open_operation(ex_img)\n",
    "    return ex_img\n",
    "\n",
    "def contrast(img):\n",
    "    _, mask = cv.threshold(img, thresh=50, maxval=255, type=cv.THRESH_OTSU)\n",
    "    ex_img = prepare_image(mask)\n",
    "    res = cv.bitwise_and(img, ex_img)\n",
    "    # cv.imshow(\"masked\", res)\n",
    "    # cv.imshow(\"mask\", mask)\n",
    "    # cv.imshow(\"unmasked\", img)\n",
    "    return res\n",
    "\n",
    "def saveImg(img):\n",
    "  im = Image.fromarray((img*255))\n",
    "  plt.imshow(im, cmap = 'gray')\n",
    "  if im.mode !='RGB':\n",
    "    im = im.convert('RGB')\n",
    "  im.save(f'./custom_dataset/A/{i}.png')\n",
    "\n",
    "print(len(frames))\n",
    "processedFrames= []\n",
    "for i, frame in enumerate(frames):\n",
    "\n",
    "  img = resize(frame, (150,150,1))\n",
    "  img *= 255\n",
    "  img = img.astype(np.uint8)\n",
    "  img = contrast(img)\n",
    "  plt.imshow(img, cmap = 'gray')  \n",
    "  # print(frame.type())\n",
    "  x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    " \n",
    "  processedFrames.append(x)\n",
    "  saveImg(x)\n",
    "\n",
    "\n",
    "  # for i in range(len(predictions[0])):\n",
    "  #   print(classes[i], predictions[0][i])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
