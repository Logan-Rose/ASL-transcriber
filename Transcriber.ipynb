{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASL TRANSCRIBER\n",
    "\n",
    "CSI 4106 - Introduction to Artificial Intelligence <br>\n",
    "Prof. Prasadith Buddhitha Kirinde Gamaarachchige <br>\n",
    "School of Electrical Engineering and Computer Science <br>\n",
    "University of Ottawa<br> \n",
    "\n",
    "Gabrelle Duenas - 300081950 <br>\n",
    "Jusley Xavier Amani Mutangana - 300094632 <br>\n",
    "Logan Rose - 300059034 <br>\n",
    "\n",
    "Run each cell in this notebook to use the ASL transcriber\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "c:\\programdata\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
       "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
       "c:\\programdata\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
       "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
       "c:\\programdata\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
       "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
       "c:\\programdata\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
       "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
       "c:\\programdata\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
       "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
       "c:\\programdata\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
       "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
       "2022-04-22 20:50:54.240273: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
       "2022-04-22 20:50:54.240903: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
       "Traceback (most recent call last):\n",
       "  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
       "    return _run_code(code, main_globals, None,\n",
       "  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
       "    exec(code, run_globals)\n",
       "  File \"C:\\ProgramData\\Anaconda3\\Scripts\\tensorboard.exe\\__main__.py\", line 7, in <module>\n",
       "  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorboard\\main.py\", line 57, in run_main\n",
       "    tensorboard = program.TensorBoard(\n",
       "  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorboard\\program.py\", line 144, in __init__\n",
       "    self.plugin_loaders = [make_loader(p) for p in plugins]\n",
       "  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorboard\\program.py\", line 144, in <listcomp>\n",
       "    self.plugin_loaders = [make_loader(p) for p in plugins]\n",
       "  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorboard\\program.py\", line 143, in make_loader\n",
       "    raise ValueError(\"Not a TBLoader or TBPlugin subclass: %s\" % plugin)\n",
       "ValueError: Not a TBLoader or TBPlugin subclass: <class 'tensorboard_plugin_wit.wit_plugin_loader.WhatIfToolPluginLoader'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons import image\n",
    "import skimage\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "TRAIN_NEW = False\n",
    "\n",
    "SEED = 123\n",
    "tf.random.set_seed(SEED)\n",
    "classes=['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "\n",
    "# load tensorboard extension\n",
    "%reload_ext tensorboard\n",
    "# specify the log directory where the tensorboard logs will be written\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10991 images belonging to 26 classes.\n",
      "Found 2734 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "img_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=30.0,\n",
    "    width_shift_range=25.0,\n",
    "    height_shift_range=25.0,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=.2,\n",
    "    fill_mode='constant',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    validation_split=0.2    \n",
    ")\n",
    "\n",
    "train_generator = img_generator.flow_from_directory(\n",
    "  './asl_dataset',\n",
    "  target_size=(150,150),\n",
    "  color_mode='grayscale',\n",
    "  classes=classes,\n",
    "  class_mode='categorical',\n",
    "  seed=SEED,\n",
    "  shuffle=False,\n",
    "  subset='training'\n",
    ")\n",
    "\n",
    "val_generator = img_generator.flow_from_directory(\n",
    "  './asl_dataset',\n",
    "  target_size=(150,150),\n",
    "  color_mode='grayscale',\n",
    "  classes=classes,\n",
    "  class_mode='categorical',\n",
    "  seed=SEED,\n",
    "  shuffle=False,\n",
    "  subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is reused from Assignment 2 to make logs and Models directories\n",
    "def make_directories():\n",
    "    d = datetime.today()\n",
    "    timestamp = d.strftime('%Y%m%d_%H%M%S')\n",
    "    # folder to store the tensorboard logs\n",
    "    tensorlog_folder = os.path.join(os.path.curdir, 'logs', timestamp)\n",
    "    # folder to store the trained models\n",
    "    checkpoint_folder = os.path.join(os.path.curdir, 'models', timestamp)\n",
    "\n",
    "    os.mkdir(tensorlog_folder)\n",
    "    os.mkdir(checkpoint_folder)\n",
    "\n",
    "    return checkpoint_folder, tensorlog_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dir, tboard_dir = make_directories()\n",
    "\n",
    "# If you would like to train the model yourself, set TRAIN_NEW to false\n",
    "if TRAIN_NEW:\n",
    "  input = keras.Input(shape=(150, 150, 1))\n",
    "  layer1 = tf.keras.layers.Conv2D(16, (4,4), activation= 'relu', name=\"layer1\")\n",
    "  layer2 = tf.keras.layers.Conv2D(16, (4,4), activation= 'relu', name=\"layer2\")\n",
    "  layer3 = tf.keras.layers.MaxPool2D((2,2), name=\"layer3\")\n",
    "  layer4 = tf.keras.layers.Dropout(0.5,name=\"layer4\")\n",
    "  layer5 = tf.keras.layers.Conv2D(32, (4,4), activation= 'relu', name=\"layer5\")\n",
    "  layer6 = tf.keras.layers.Conv2D(32, (4,4), activation= 'relu', name=\"layer6\")\n",
    "  layer7 = tf.keras.layers.MaxPool2D((2,2), name=\"layer7\")\n",
    "  layer8 = tf.keras.layers.Dropout(0.5,name=\"layer8\")\n",
    "  layer9 = tf.keras.layers.Flatten(name=\"layer9\")\n",
    "  layer10 = tf.keras.layers.Dense(128, activation='relu', name='layer10')\n",
    "  layer11 = tf.keras.layers.Dense(128, activation='relu', name='layer11')\n",
    "\n",
    "  output =  tf.keras.layers.Dense(len(classes), activation='softmax', name='symbol')\n",
    "\n",
    "  network = output(layer11(layer10(layer9(layer8(layer7(layer7(layer6(layer5(layer4(layer3(layer3(layer2(layer1(input))))))))))))))\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "  model = tf.keras.Model(inputs=input, outputs=network, name=\"ASL Classifier\")\n",
    "\n",
    "  model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                loss_weights=[0.01],\n",
    "                metrics=[tf.keras.metrics.top_k_categorical_accuracy, 'accuracy'],\n",
    "                )\n",
    "\n",
    "  early_stop = keras.callbacks.EarlyStopping(\n",
    "      monitor='val_loss',\n",
    "      patience=8,\n",
    "      restore_best_weights=True\n",
    "  )\n",
    "  checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "      filepath=check_dir,\n",
    "      monitor='val_loss',\n",
    "      save_best_only=True,\n",
    "      save_weights_only=False\n",
    "  )\n",
    "  tensorboard = keras.callbacks.TensorBoard(\n",
    "      log_dir=tboard_dir\n",
    "  )\n",
    "  reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "      monitor='val_loss',\n",
    "      factor=0.1,\n",
    "      patience=5,\n",
    "      min_lr=1e-5,\n",
    "      verbose=1\n",
    "  )\n",
    "\n",
    "  callbacks = [checkpoints, early_stop, tensorboard, reduce_lr]\n",
    "  model.fit(\n",
    "          train_generator,\n",
    "          epochs=100,\n",
    "          validation_data=val_generator,\n",
    "          batch_size=16, \n",
    "          callbacks=callbacks,\n",
    "          )\n",
    "else:\n",
    "  model = tf.keras.models.load_model(f'./models/20220422_105517')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 520 images belonging to 26 classes.\n",
      "17/17 [==============================] - 5s 284ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.70      0.39        20\n",
      "           1       1.00      0.45      0.62        20\n",
      "           2       0.94      0.80      0.86        20\n",
      "           3       0.86      0.60      0.71        20\n",
      "           4       0.88      0.35      0.50        20\n",
      "           5       0.94      0.80      0.86        20\n",
      "           6       1.00      0.85      0.92        20\n",
      "           7       0.90      0.95      0.93        20\n",
      "           8       1.00      0.35      0.52        20\n",
      "           9       0.56      0.45      0.50        20\n",
      "          10       0.75      0.60      0.67        20\n",
      "          11       0.49      0.95      0.64        20\n",
      "          12       0.55      0.60      0.57        20\n",
      "          13       0.57      0.20      0.30        20\n",
      "          14       0.93      0.65      0.76        20\n",
      "          15       0.95      0.90      0.92        20\n",
      "          16       0.94      0.80      0.86        20\n",
      "          17       0.71      0.50      0.59        20\n",
      "          18       0.38      1.00      0.55        20\n",
      "          19       0.39      0.80      0.52        20\n",
      "          20       0.75      0.45      0.56        20\n",
      "          21       0.65      0.65      0.65        20\n",
      "          22       1.00      0.85      0.92        20\n",
      "          23       0.80      0.80      0.80        20\n",
      "          24       0.92      0.60      0.73        20\n",
      "          25       0.72      0.65      0.68        20\n",
      "\n",
      "    accuracy                           0.67       520\n",
      "   macro avg       0.76      0.67      0.67       520\n",
      "weighted avg       0.76      0.67      0.67       520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the Imagedatagenerator for the testing data\n",
    "test_img_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=25.0,\n",
    "    height_shift_range=25.0,\n",
    "    horizontal_flip=True, \n",
    ")\n",
    "test_generator = test_img_generator.flow_from_directory(\n",
    "  './test_data',\n",
    "  target_size=(150,150),\n",
    "  color_mode='grayscale',\n",
    "  classes=classes,\n",
    "  class_mode='categorical',\n",
    "  seed=SEED,\n",
    "  shuffle=False,\n",
    ")\n",
    "\n",
    "y_pred = model.predict(test_generator, batch_size=64, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(test_generator.classes, y_pred_bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture Hand Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use only one hand\n",
      "Please use only one hand\n",
      "Please use only one hand\n",
      "Please use only one hand\n",
      "Please use only one hand\n",
      "Please use only one hand\n",
      "Please use only one hand\n",
      "Please use only one hand\n",
      "Please use only one hand\n"
     ]
    }
   ],
   "source": [
    "# takes hand detection output to add bounding rectangles\n",
    "def find_hands(current, width, height):\n",
    "    rect_list = []\n",
    "    hand_classifier = []\n",
    "    center = (0,0)\n",
    "    # iterates through hand tracking data per hand\n",
    "    if current.multi_hand_landmarks:\n",
    "        for index, info in enumerate(current.multi_handedness):\n",
    "            which = info.classification[0].label\n",
    "            hand_classifier.append(which)\n",
    "        for index, hand in enumerate(current.multi_hand_landmarks):\n",
    "            # values from 0 to 1 converted into values corresponding to video size\n",
    "            hand_list = []\n",
    "            for individual in hand.landmark:\n",
    "                hand_list.append((int(individual.x * width), int(individual.y * height), int(individual.z * width)))\n",
    "            # developing bounding box coordinates\n",
    "            x_values = np.array(hand_list)[:, 0]\n",
    "            y_values = np.array(hand_list)[:, 1]\n",
    "            x_min = int(np.min(x_values) - 10)\n",
    "            y_min = int(np.min(y_values) - 10)\n",
    "            x_max = int(np.max(x_values) + 10)\n",
    "            y_max = int(np.max(y_values) + 10)\n",
    "            center = (x_min + (x_max-x_min)//2, y_min + (y_max-y_min)//2)\n",
    "            rect_list.append(((x_min, y_min), (x_max, y_max), (0, 255, 0), hand_classifier[index]))\n",
    "    return rect_list, center\n",
    "def capture():\n",
    "    video = cv.VideoCapture(0, cv.CAP_DSHOW)\n",
    "    running, original = video.read()\n",
    "    h, w, _ = original.shape    \n",
    "    # machine learning algorithm (using mediapipe, via google)\n",
    "    hands = mp.solutions.hands.Hands()\n",
    "    savedFrames = []\n",
    "    # continuous looping\n",
    "    while True:\n",
    "        running, original = video.read()\n",
    "        if not running:\n",
    "            break\n",
    "        # hand processing\n",
    "        track_curr = hands.process(original)\n",
    "        # calculating bounding rectangles\n",
    "        current_rects, center = find_hands(track_curr, w, h)\n",
    "\n",
    "        if current_rects:\n",
    "          if len(current_rects) > 1:\n",
    "            print('Please use only one hand')\n",
    "          else:\n",
    "            current_rect = current_rects[0]\n",
    "            left = current_rect[0][0]\n",
    "            right = current_rect[1][0]\n",
    "            top = current_rect[0][1]\n",
    "            bottom = current_rect[1][1]\n",
    "            hand_width = right - left\n",
    "            hand_height = bottom-top\n",
    "            square_top_left = (center[0]-75,center[1]-75)\n",
    "            square_bottom_right = (center[0]+75,center[1]+75)\n",
    "            red = (0,0,255)\n",
    "            green = (0,255,0)\n",
    "            if hand_width <= 150 and hand_height <= 150:\n",
    "              if top > 0 and left > 0 and bottom > 0 and right > 0:\n",
    "                cv.rectangle(original,square_top_left ,square_bottom_right , green,1)\n",
    "                crop = original[center[1]-75:center[1]+75,center[0]-75:center[0]+75]   \n",
    "                savedFrames.append(crop)\n",
    "            else:\n",
    "              cv.rectangle(original, current_rect[0], current_rect[1], red, 2)\n",
    "              cv.rectangle(original,square_top_left ,square_bottom_right , red,2)\n",
    "        cv.imshow(\"Video Feed\", original)\n",
    "        k = cv.waitKey(1) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "    cv.destroyAllWindows()\n",
    "    video.release()\n",
    "    return savedFrames\n",
    "frames = capture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Hand Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames classified: ggjjjjjjjjjjjj\n",
      "Final: j\n"
     ]
    }
   ],
   "source": [
    "processedFrames= []\n",
    "for i, frame in enumerate(frames):\n",
    "  img = resize(frame, (150,150,1))\n",
    "  img *= 255\n",
    "  img = img.astype(np.uint8)\n",
    "  x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  processedFrames.append(x)\n",
    "images = np.vstack(processedFrames)\n",
    "predictions = model.predict(images)\n",
    "output = \"\"\n",
    "for prediction in predictions:\n",
    "  classIndex = np.argmax(prediction)\n",
    "  output += classes[classIndex]\n",
    "\n",
    "strengths = [(output[0],1)]\n",
    "for i in range(1, len(output)):\n",
    "  if output[i] == output[i-1]:\n",
    "    strengths[-1] = (strengths[-1][0],strengths[-1][1]+1)\n",
    "  else:\n",
    "    strengths.append((output[i],1))\n",
    "\n",
    "barrier = 10\n",
    "final = ''\n",
    "\n",
    "for letter in strengths:\n",
    "  if letter[1] >= barrier:\n",
    "    if len(final) > 0:\n",
    "      if letter[0] != final[-1]:\n",
    "        final += letter[0]\n",
    "    else:\n",
    "      final += letter[0]\n",
    "\n",
    "pureStrengths = []\n",
    "for i in strengths:\n",
    "  pureStrengths.append(i[1])\n",
    "plt.plot(pureStrengths)\n",
    "plt.axhline(y=barrier, color='r', linestyle='-')\n",
    "plt.show()\n",
    "\n",
    "print(f'Frames classified: {output}')\n",
    "print(f'Final: {final}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "367e97bd1bb993270875333a5c0c0ed703268d8b20e00e99d22f213f8fc7c5a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
